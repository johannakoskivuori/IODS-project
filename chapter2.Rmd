# 2. Regression and model validation

This week I performed regression analysis and method validation for the data learning2014 that I created in the data wrangling exercise. Below you can find the analysis that was performed, more information about the data that was analyzed and of course the results I got.


```{r}
date()

```

```{r}

students2014 <- read.table("learning2014.txt", sep = ",", header = TRUE)
students2014$gender <- factor(students2014$gender) # Making gender a factor

```

```{r}
str(students2014)
dim(students2014)
head(students2014)
```
The Data has 166 rows and 7 columns. Variables in the data are : gender, age, attitude (attitude towards statistics), deep = Deep approach, stra = Strategic approach, surf = Surface approach and points (exam points). Data is from a survey regarding teaching and learning. Variables attitude, deep, stra and surf are scaled back to the likert scale (1-5) from the [original data](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt). More information from the data can be found [here](https://www.mv.helsinki.fi/home/kvehkala/JYTmooc/JYTOPKYS3-meta.txt)


#### Graphical overview of the data


Here is one possible graphical overview of the data. In the picture we can see females and males in different colours. There are more females in the data (F=110) than males (M=56). Results between females and males are quite evenly distributed. In the summary table are the mean values of different variables and also minimum (min) and maximum (max) values. Age distribution seems to be more on the left so the most of the students are between 20-30 years, mean age 25,51. Values of attitude, stra and surf are quite evenly distributed. Distribution of deep values is a bit more on the right side and so the mean value is also quite high 3.680. If we look at the points, the distribution looks interesting. There is a small group on the left with small values (~10 points) and then a bigger group with higher points. 

```{r}
pairs(students2014[-1], col = students2014$gender)# Females and males in different colours, F=black, M=red
summary(students2014)

```

#### Linear models


I chose age, attitude and deep as explanatory variables and fitted the linear model where points is the target variable.

```{r}
model1 <- lm(points ~ age + attitude + deep, data = students2014)
summary(model1)

```
From the summary we can see the model1 parameters. Estimates tell the relationships between the explanatory variables and the target variable. By using the estimates we can also write down the linear equation: _points=15.60773-0.07716age+3.59413attitude-0.60275deep_.Then there are also standard errors in the summary, and t-values from the statistical test that was performed and p-values which tell if the explanatory variable is statistically significant variable. Usually we choose the level of significance, often it is 0,05. If we look at the variable age, we see that the p-value is 0,149 > 0,05, so age is not statistically significant variable. Variable deep is not either statistically significant variable because its p-value is even higher 0,423 > 0,05. But if we look at the variable attitude, p-value is very small 2.56e-09 < 0,05, which means that it is statistically significant. 

Next I remove age and deep from the model, because they were not statistically significant. 

```{r}
model2 <- lm(points ~  attitude, data = students2014)
summary(model2)


```
From the summary we can see now the model2 parameters which have now changed a bit because of the removal of those other variables. If we look at the relationship between the variables points and attitude, the attitude is statistically significant variable, p-value is very small 4.12e-09 < 0,05. Model equation is: _points=11.6372+3.5255attitude_. This means that when attitude increases by one, exam points will increase 3.5255 points on average. 

Multiple R-squared is:  0.1906.This means that the fitted model explains 19,06 % of the variation in exam points. In my opinion, R squared is quite low (R squared is always between 0 and 100 %), but on the other hand we have statistically significant variable, which is good.


#### Diagnostic plots


Diagnostic plots can tell us about the goodness of the fitted model. There are few assumptions related to the evaluation of fitted models. Often when analyzing fitted models we look at residuals and errors. Errors are assumed to be normally distributed and they are not correlated. They are assumed to have constant variance and the size of a error should not be dependent on the explanatory variables. These assumptions we can evaluate with a QQ-plot, a scatter plot of residuals versus model predictions and with a residuals vs leverage plot.


```{r}
plot(model2, which = c(1)) # Residuals vs fitted plot of model2

```


Residuals vs fitted plot can tell us about the constant variance of errors which is assumed. If we look at the residuals vs fitted plot, we can see that residuals are quite well distributed around the 0-line, which means that the variance of errors is constant. There are only three points which might be outliers (145, 56, 35).

```{r}
plot(model2, which = c(2)) # QQ-plot of model2

```

QQ-plot can help us figure out if the assumption of normality of the errors is true. If we look at the QQ-plot we can see that points follow the line quite nicely, so we can assume that the errors are normally distributed. 

```{r}
plot(model2, which = c(5)) # Residuals vs Leverage plot of model2

```

Leverage of observations can tell us if a single observation has a high impact on the model. If we look at the Residuals vs Leverage plot we see that the leverage scale is from 0 to 0,04, so their values are small and there arenÂ´t any outliers. So the leverage seem regular, which also means that the errors are regular. 

Based on the results of the model validity tests I would say that the model that was fitted is okay. 

